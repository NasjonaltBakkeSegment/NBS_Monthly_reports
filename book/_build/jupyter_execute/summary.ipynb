{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe68e74",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# NBS mothly report for November 2023"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "todays_date = str(datetime.now().date())\n",
    "month = datetime.now().strftime(\"%B\")\n",
    "year = str(datetime.now().year)\n",
    "\n",
    "display_markdown('''# NBS mothly report for {month} {year}''')\n",
    "md(\"# NBS mothly report for {} {}\".format(month, year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c8eea",
   "metadata": {},
   "source": [
    "# Quick summary\n",
    "\n",
    "The table below shows a short overview of the NBS preformance operation during the last 30 days. The three different FEs and the BE are included. All columns represents the number of products in each portal excepting the last 3 columns. Those 3 columns represents the data flow from MET Norway to users through the portals where Volumes are measured in Tb. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e505b4f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31293/1792926152.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_BE = data_BE.append(data_clean)\n",
      "/tmp/ipykernel_31293/1792926152.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_BE = data_BE.append(data_clean)\n",
      "/tmp/ipykernel_31293/1792926152.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_BE = data_BE.append(data_clean)\n",
      "/tmp/ipykernel_31293/1792926152.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_BE = data_BE.append(data_clean)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import copy\n",
    "from myst_nb import glue\n",
    "\n",
    "logsdir = pathlib.Path('../data')\n",
    "hubs=['colhub_global', 'scihub', 'esahub_global', 'colhub_AOI']\n",
    "products=['S1', 'S2L1C', 'S2L2A', 'S3', 'S5p']\n",
    "\n",
    "data_FE = None\n",
    "for h in hubs:\n",
    "    csvfile = logsdir / f'products_in_{h}.csv'\n",
    "    data_tmp = pd.read_csv(csvfile, header=None, names=['product', 'area', 'sensing_date', f'{h}'], parse_dates=['sensing_date'])\n",
    "    # If several sensing date exist, keep the most recent one\n",
    "    data_clean = copy.deepcopy(data_tmp.drop_duplicates(subset=['sensing_date', 'product', 'area'], keep='last'))\n",
    "    if h == 'colhub_AOI':\n",
    "        data_clean['area'] = 'colhub_aoi'\n",
    "    if data_FE is None:\n",
    "        data_FE = data_clean\n",
    "    else:\n",
    "        data_FE = data_FE.merge(data_clean, on=['sensing_date', 'product', 'area'], how='outer')\n",
    "        \n",
    "data_BE = None\n",
    "areas = ['AOI']\n",
    "for a in areas:\n",
    "    for p in products:\n",
    "        csvfile = logsdir / f'products_in_BE_{p}_{a}.csv'\n",
    "        data_tmp = pd.read_csv(csvfile, header=None, names=['product', 'area', 'sensing_date', 'BE'], parse_dates=['sensing_date'])\n",
    "        # If several sensing date exist, keep the most recent one\n",
    "        data_clean = copy.deepcopy(data_tmp.drop_duplicates(subset=['sensing_date', 'product', 'area'], keep='last'))\n",
    "        if a == 'AOI':\n",
    "            data_clean['area'] = 'colhub_aoi'\n",
    "        if data_BE is None:\n",
    "            data_BE = data_clean\n",
    "        else:\n",
    "            data_BE = data_BE.append(data_clean)\n",
    "            \n",
    "data = data_FE.merge(data_BE, on=['sensing_date', 'product', 'area'], how='outer')\n",
    "data.set_index('sensing_date', inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "mask_s1 = (data['product'] == 'S1') & (data['area'] == 'colhub_aoi')\n",
    "data_s1 = data.loc[mask_s1][-30:][['colhub_global', 'scihub', 'colhub_AOI', 'BE']].copy()\n",
    "col_s1 = data_s1['colhub_global'].sum()\n",
    "arc_s1 = data_s1['colhub_AOI'].sum()\n",
    "sci_s1 = data_s1['scihub'].sum()\n",
    "be_s1 = data_s1['BE'].sum()\n",
    "\n",
    "mask_s2l1 = (data['product'] == 'S2L1C') & (data['area'] == 'colhub_aoi')\n",
    "data_s2l1 = data.loc[mask_s2l1][-30:][['colhub_global', 'scihub', 'colhub_AOI', 'BE']].copy()\n",
    "col_s2l1 = data_s2l1['colhub_global'].sum()\n",
    "arc_s2l1 = data_s2l1['colhub_AOI'].sum()\n",
    "sci_s2l1 = data_s2l1['scihub'].sum()\n",
    "be_s2l1 = data_s2l1['BE'].sum()\n",
    "\n",
    "mask_s2l2 = (data['product'] == 'S2L2A') & (data['area'] == 'colhub_aoi')\n",
    "data_s2l2 = data.loc[mask_s2l2][-30:][['colhub_global', 'scihub', 'colhub_AOI', 'BE']].copy()\n",
    "col_s2l2 = data_s2l2['colhub_global'].sum()\n",
    "arc_s2l2 = data_s2l2['colhub_AOI'].sum()\n",
    "sci_s2l2 = data_s2l2['scihub'].sum()\n",
    "be_s2l2 = data_s2l2['BE'].sum()\n",
    "\n",
    "mask_s3 = (data['product'] == 'S3') & (data['area'] == 'colhub_aoi')\n",
    "data_s3 = data.loc[mask_s3][-30:][['colhub_global', 'scihub', 'colhub_AOI', 'BE']].copy()\n",
    "col_s3 = data_s3['colhub_global'].sum()\n",
    "arc_s3 = data_s3['colhub_AOI'].sum()\n",
    "sci_s3 = data_s3['scihub'].sum()\n",
    "be_s3 = data_s3['BE'].sum()\n",
    "\n",
    "mask_s5 = (data['product'] == 'S5p') & (data['area'] == 'colhub_aoi')\n",
    "data_s5 = data.loc[mask_s5][-30:][['colhub_global', 'scihub', 'colhub_AOI', 'BE']].copy()\n",
    "col_s5 = data_s5['colhub_global'].sum()\n",
    "arc_s5 = data_s5['colhub_AOI'].sum()\n",
    "sci_s5 = data_s5['scihub'].sum()\n",
    "be_s5 = data_s5['BE'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659a82a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 5 fields in line 248551, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m col_vol \u001b[38;5;241m=\u001b[39m nbs_30\u001b[38;5;241m.\u001b[39msum()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m \u001b[38;5;66;03m#in Tb\u001b[39;00m\n\u001b[1;32m     43\u001b[0m csvfile \u001b[38;5;241m=\u001b[39m logsdir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNBS_frontend-AOI_outputs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 44\u001b[0m nbs_AOI \u001b[38;5;241m=\u001b[39m get_data(csvfile)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#nbs_AOI.set_index('download_time', inplace=True)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m nbs_AOI\u001b[38;5;241m.\u001b[39msort_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(file):\n\u001b[0;32m---> 25\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_duration\u001b[39m\u001b[38;5;124m'\u001b[39m]\\\n\u001b[1;32m     26\u001b[0m                         , parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_time\u001b[39m\u001b[38;5;124m'\u001b[39m], index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msatellite\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     28\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_product_type)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 5 fields in line 248551, saw 7\n"
     ]
    }
   ],
   "source": [
    "def get_product_type(product):\n",
    "    if product[0:2] == 'S1':\n",
    "        type = product.split('_')[2]\n",
    "    elif product[0:2] == 'S2':\n",
    "        type = product.split('_')[1]\n",
    "        if not type.startswith('M'):\n",
    "            type = 'Unknown'\n",
    "    elif product[0:2] == 'S3':\n",
    "        tmp = product.split('_')\n",
    "        if tmp[1] == 'SL':\n",
    "            type = 'SLSTR_L' + tmp[2]\n",
    "        elif tmp[1] == 'SR':\n",
    "            type = 'SRAL_L' + tmp[2]\n",
    "        elif tmp[1] == 'OL':\n",
    "            type = 'OLCI_L' + tmp[2]\n",
    "        else:\n",
    "            type = 'Unknown'\n",
    "    else:\n",
    "        type = 'Unknown'\n",
    "    if 'DTERRENG' in product:\n",
    "        type = type + '_DTERRENG'\n",
    "    return type\n",
    "\n",
    "def get_data(file):\n",
    "    data = pd.read_csv(file, header=None, names=['download_time', 'user', 'product', 'size', 'download_duration']\\\n",
    "                        , parse_dates=['download_time'], index_col='download_time')\n",
    "    data['satellite'] = data['product'].apply(lambda x: x[0:2])\n",
    "    data['product_type'] = data['product'].apply(get_product_type)\n",
    "    return data[data['product_type'] != 'Unknown']\n",
    "\n",
    "csvfile = logsdir / 'NBS_frontend-global_outputs.csv'\n",
    "nbs_global = get_data(csvfile)\n",
    "#nbs_global.set_index('download_time', inplace=True)\n",
    "nbs_global.sort_index(inplace=True)\n",
    "nbs_global.fillna(0, inplace=True)\n",
    "nbs_30 = nbs_global.loc[nbs_global.index >= dt.datetime.today() - dt.timedelta(days=30)].copy()\n",
    "\n",
    "col_use = len(nbs_30['user'].unique())\n",
    "col_nb = nbs_30.count()['product_type']\n",
    "col_vol = nbs_30.sum()['size']/1024/1024/1024/1024 #in Tb\n",
    "\n",
    "\n",
    "csvfile = logsdir / 'NBS_frontend-AOI_outputs.csv'\n",
    "nbs_AOI = get_data(csvfile)\n",
    "#nbs_AOI.set_index('download_time', inplace=True)\n",
    "nbs_AOI.sort_index(inplace=True)\n",
    "nbs_AOI.fillna(0, inplace=True)\n",
    "aoi_30 = nbs_AOI.loc[nbs_AOI.index >= dt.datetime.today() - dt.timedelta(days=30)].copy()\n",
    "\n",
    "arc_use = len(aoi_30['user'].unique())\n",
    "arc_nb = aoi_30.count()['product_type']\n",
    "arc_vol = aoi_30.sum()['size']/1024/1024/1024/1024 #in Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c774f6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def read_dhus_logs_details(file):\n",
    "    data = pd.read_csv(file, header=None, names=['day', 'product_type', 'action', 'volume', 'number', 'timeliness']\\\n",
    "                        , parse_dates=['day'], index_col=['day'])\n",
    "    return data\n",
    "\n",
    "def read_csv(file):\n",
    "    data_tmp = pd.read_csv(file, header=None, names=['product_type', 'day', 'number', 'volume'], parse_dates=['day'], sep=';')\n",
    "    # If several sensing date exist, keep the numbers only for the most recent script run (ie highest day index)\n",
    "    data_clean = copy.deepcopy(data_tmp.drop_duplicates(subset=['product_type', 'day'], keep='last')).set_index('day').sort_index()\n",
    "    data_clean['volume'] = data_clean['volume']/1024./1024\n",
    "    return data_clean\n",
    "\n",
    "data_s1 = read_dhus_logs_details(logsdir / 'S1-backend-AOI_inputs.csv')\n",
    "data_s2l1c = read_dhus_logs_details(logsdir / 'S2L1C-backend-AOI_inputs.csv')\n",
    "data_s2l2a = read_dhus_logs_details(logsdir / 'S2L2A-backend-AOI_inputs.csv')\n",
    "data_s3 = read_dhus_logs_details(logsdir / 'S3-backend-AOI_inputs.csv')\n",
    "data_s5 = read_dhus_logs_details(logsdir / 'S5p-backend-AOI_inputs.csv')\n",
    "all_colhub = data_s1.append(data_s2l1c).append(data_s2l2a).append(data_s3).append(data_s5)\n",
    "\n",
    "csvfile = pathlib.Path('../data/nb_products_volume_per_sensing_date.csv')\n",
    "all_netcdf = read_csv(csvfile)\n",
    "\n",
    "alle = pd.concat([all_colhub, all_netcdf])\n",
    "\n",
    "all_vol_be = alle['volume'].sum()/1024 #in Tb\n",
    "\n",
    "lately = alle[alle.index >= dt.datetime.today() - dt.timedelta(days=30)]\n",
    "vol_be = lately['volume'].sum()/1024 #in Tb\n",
    "nb_be = be_s1 + be_s2l1 + be_s2l2 + be_s3 + be_s5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc26074",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "glue(\"col_s1\", col_s1)\n",
    "glue(\"col_s2l1\", col_s2l1)\n",
    "glue(\"col_s2l2\", col_s2l2)\n",
    "glue(\"col_s3\", col_s3)\n",
    "glue(\"col_s5\", col_s5)\n",
    "glue(\"arc_s1\", arc_s1)\n",
    "glue(\"arc_s2l1\", arc_s2l1)\n",
    "glue(\"arc_s2l2\", arc_s2l2)\n",
    "glue(\"arc_s3\", arc_s3)\n",
    "glue(\"arc_s5\", arc_s5)\n",
    "glue(\"sci_s1\", sci_s1)\n",
    "glue(\"sci_s2l1\", sci_s2l1)\n",
    "glue(\"sci_s2l2\", sci_s2l2)\n",
    "glue(\"sci_s3\", sci_s3)\n",
    "glue(\"sci_s5\", sci_s5)\n",
    "glue(\"be_s1\", be_s1)\n",
    "glue(\"be_s2l1\", be_s2l1)\n",
    "glue(\"be_s2l2\", be_s2l2)\n",
    "glue(\"be_s3\", be_s3)\n",
    "glue(\"be_s5\", be_s5)\n",
    "glue(\"col_use\", col_use)\n",
    "glue(\"col_nb\", col_nb)\n",
    "glue(\"col_vol\", col_vol)\n",
    "glue(\"arc_use\", arc_use)\n",
    "glue(\"arc_nb\", arc_nb)\n",
    "glue(\"arc_vol\", arc_vol)\n",
    "glue(\"nb_be\", nb_be)\n",
    "glue(\"vol_be\", vol_be)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99769f48",
   "metadata": {},
   "source": [
    "| Portals | S1 | S2L1C | S2L2A | S3 | S5p | Nb of users | Nb of products | Volume |\n",
    "|:---------:|:---------:|:--------:|:---------:|:----------:|:----------:|:-------------:|:---------:|:----------:|\n",
    "| colhub.met.no | {glue:text}`col_s1:.0f` | {glue:text}`col_s2l1:.0f` | {glue:text}`col_s2l2:.0f` | {glue:text}`col_s3:.0f` | {glue:text}`col_s5:.0f` | {glue:text}`col_use` | {glue:text}`col_nb` | {glue:text}`col_vol:.3f` |\n",
    "| colhub-archive.met.no | {glue:text}`arc_s1:.0f` | {glue:text}`arc_s2l1:.0f` | {glue:text}`arc_s2l2:.0f` | {glue:text}`arc_s3:.0f` | {glue:text}`arc_s5:.0f` | {glue:text}`arc_use` | {glue:text}`arc_nb` | {glue:text}`arc_vol:.3f` |\n",
    "| scihub.copernicus.eu | {glue:text}`sci_s1:.0f` | {glue:text}`sci_s2l1:.0f` | {glue:text}`sci_s2l2:.0f` | {glue:text}`sci_s3:.0f` | {glue:text}`sci_s5:.0f` | - | - | - |\n",
    "| MET Norway BE | {glue:text}`be_s1:.0f` | {glue:text}`be_s2l1:.0f` | {glue:text}`be_s2l2:.0f` | {glue:text}`be_s3:.0f` | {glue:text}`be_s5:.0f` | - | - | - |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639eaf17",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(\"Finally, the total amount of disk space dedicated to the NBS project, including either products in SAFE and NetCDF formats, represents {} Tb.\".format(int(all_vol_be)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1d3ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Monthly volumes (in Tb)\n",
    "lately = alle[alle.index >= dt.datetime.today() - dt.timedelta(days=365)]\n",
    "year = (lately['volume'].sum()/1024.)\n",
    "after_year = year + all_vol_be\n",
    "short = lately[lately.index <= dt.datetime.today() - dt.timedelta(days=123)]\n",
    "m6 = (short['volume'].sum()/1024.)\n",
    "after_6m = m6 + all_vol_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df449b93",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(\"Due to tracking the data ingested and produced for the NBS project in the last year it is possible to forcast the upcoming need for disk space. As long as data flows follows the same pattern than last year, in 6 months the total disk space will grow until {} Tb; while in 12 months it is forecasted to become {} Tb.\".format(int(after_6m), int(after_year)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}