{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe68e74",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display_markdown\n",
    "import pandas as pd\n",
    "\n",
    "todays_date = str(datetime.now().date())\n",
    "month = datetime.now().strftime(\"%B\")\n",
    "year = str(datetime.now().year)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../utils/'))\n",
    "from shared_functions import get_year_and_month, get_month_name\n",
    "\n",
    "year, month = get_year_and_month()\n",
    "month_name = get_month_name(month)\n",
    "\n",
    "# Create start and end dates\n",
    "start_date = pd.to_datetime(f'{year}-{month}-01')\n",
    "end_date = pd.to_datetime(f'{year}-{month + 1}-01') - pd.Timedelta(days=1)  # Last day of the month\n",
    "last_year = pd.to_datetime(f'{year-1}-{month + 1}-01') - pd.Timedelta(days=1) # for 1 year of data\n",
    "date_123 = end_date - pd.Timedelta(days=123) # For 123 days of data\n",
    "# display_markdown('''# NBS monthly report for {month} {year}''')\n",
    "# md(\"# NBS monthly report for {} {}\".format(month, year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c8eea",
   "metadata": {},
   "source": [
    "# Quick summary\n",
    "\n",
    "The table below shows a short overview of the NBS performance operation during the last 30 days. The number of products are compared against CDSE. All columns represents the number of products in each portal except the last 3 columns. Those 3 columns represents the data flow from MET Norway to users through the portals where Volumes are measured in Tb. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e505b4f4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import copy\n",
    "from myst_nb import glue\n",
    "\n",
    "logsdir = pathlib.Path('../data')\n",
    "hubs = ['colhub_global', 'cdse', 'esahub_global', 'colhub_AOI']\n",
    "products = ['S1', 'S2L1C', 'S2L2A', 'S3', 'S5p']\n",
    "\n",
    "data_FE = None\n",
    "for h in hubs:\n",
    "    csvfile = logsdir / f'products_in_{h}.csv'\n",
    "    data_tmp = pd.read_csv(csvfile, header=None, names=['product', 'area', 'sensing_date', f'{h}'], parse_dates=['sensing_date'])\n",
    "    # If several sensing date exist, keep the most recent one\n",
    "    data_clean = copy.deepcopy(data_tmp.drop_duplicates(subset=['sensing_date', 'product', 'area'], keep='last'))\n",
    "    if h == 'colhub_AOI':\n",
    "        data_clean['area'] = 'colhub_aoi'\n",
    "    if data_FE is None:\n",
    "        data_FE = data_clean\n",
    "    else:\n",
    "        data_FE = data_FE.merge(data_clean, on=['sensing_date', 'product', 'area'], how='outer')\n",
    "\n",
    "data = data_FE\n",
    "data.set_index('sensing_date', inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "data = data[\n",
    "    (data.index >= start_date) & \n",
    "    (data.index <= end_date) & \n",
    "    (data['area'] == 'colhub_aoi')\n",
    "]\n",
    "\n",
    "data_s1 = data[\n",
    "    (data['product'] == 'S1')\n",
    "][['colhub_global', 'cdse', 'colhub_AOI']].copy()\n",
    "col_s1 = data_s1['colhub_global'].sum()\n",
    "arc_s1 = data_s1['colhub_AOI'].sum()\n",
    "cdse_s1 = data_s1['cdse'].sum()\n",
    "\n",
    "data_s2l1 = data[\n",
    "    (data['product'] == 'S2L1C')\n",
    "][['colhub_global', 'cdse', 'colhub_AOI']].copy()\n",
    "col_s2l1 = data_s2l1['colhub_global'].sum()\n",
    "arc_s2l1 = data_s2l1['colhub_AOI'].sum()\n",
    "cdse_s2l1 = data_s2l1['cdse'].sum()\n",
    "\n",
    "data_s2l2 = data[\n",
    "    (data['product'] == 'S2L2A')\n",
    "][['colhub_global', 'cdse', 'colhub_AOI']].copy()\n",
    "col_s2l2 = data_s2l2['colhub_global'].sum()\n",
    "arc_s2l2 = data_s2l2['colhub_AOI'].sum()\n",
    "cdse_s2l2 = data_s2l2['cdse'].sum()\n",
    "\n",
    "data_s3 = data[\n",
    "    (data['product'] == 'S3')\n",
    "][['colhub_global', 'cdse', 'colhub_AOI']].copy()\n",
    "col_s3 = data_s3['colhub_global'].sum()\n",
    "arc_s3 = data_s3['colhub_AOI'].sum()\n",
    "cdse_s3 = data_s3['cdse'].sum()\n",
    "\n",
    "data_s5 = data[\n",
    "    (data['product'] == 'S5p')\n",
    "][['colhub_global', 'cdse', 'colhub_AOI']].copy()\n",
    "col_s5 = data_s5['colhub_global'].sum()\n",
    "arc_s5 = data_s5['colhub_AOI'].sum()\n",
    "cdse_s5 = data_s5['cdse'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659a82a1",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def get_product_type(product):\n",
    "    if product[0:2] == 'S1':\n",
    "        type = product.split('_')[2]\n",
    "    elif product[0:2] == 'S2':\n",
    "        type = product.split('_')[1]\n",
    "        if not type.startswith('M'):\n",
    "            type = 'Unknown'\n",
    "    elif product[0:2] == 'S3':\n",
    "        tmp = product.split('_')\n",
    "        if tmp[1] == 'SL':\n",
    "            type = 'SLSTR_L' + tmp[2]\n",
    "        elif tmp[1] == 'SR':\n",
    "            type = 'SRAL_L' + tmp[2]\n",
    "        elif tmp[1] == 'OL':\n",
    "            type = 'OLCI_L' + tmp[2]\n",
    "        else:\n",
    "            type = 'Unknown'\n",
    "    else:\n",
    "        type = 'Unknown'\n",
    "    if 'DTERRENG' in product:\n",
    "        type = type + '_DTERRENG'\n",
    "    return type\n",
    "\n",
    "def get_data(file):\n",
    "    data = pd.read_csv(file, header=None, names=['download_time', 'user', 'product', 'product_type', 'size', 'download_duration', 'not_avail']\\\n",
    "                        , parse_dates=['download_time'], index_col='download_time')\n",
    "    data['satellite'] = data['product'].apply(lambda x: x[0:2])\n",
    "    data['product_type'] = data['product'].apply(get_product_type)\n",
    "    return data[data['product_type'] != 'Unknown']\n",
    "\n",
    "csvfile = logsdir / 'NBS_frontend-global_outputs.csv'\n",
    "nbs_global = get_data(csvfile)\n",
    "nbs_global.sort_index(inplace=True)\n",
    "nbs_global.fillna(0, inplace=True)\n",
    "nbs_30 = nbs_global[\n",
    "    (nbs_global.index >= start_date) & \n",
    "    (nbs_global.index <= end_date)\n",
    "]\n",
    "\n",
    "col_use = len(nbs_30['user'].unique())\n",
    "col_nb = nbs_30.count()['product_type']\n",
    "col_vol = nbs_30.sum()['size']/1024/1024/1024/1024 #in Tb\n",
    "\n",
    "csvfile = logsdir / 'NBS_frontend-AOI_outputs.csv'\n",
    "nbs_AOI = get_data(csvfile)\n",
    "nbs_AOI.sort_index(inplace=True)\n",
    "nbs_AOI.fillna(0, inplace=True)\n",
    "aoi_30 = nbs_AOI[\n",
    "    (nbs_AOI.index >= start_date) & \n",
    "    (nbs_AOI.index <= end_date)\n",
    "]\n",
    "\n",
    "arc_use = len(aoi_30['user'].unique())\n",
    "arc_nb = aoi_30.count()['product_type']\n",
    "arc_vol = aoi_30.sum()['size']/1024/1024/1024/1024 #in Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c774f6d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import copy\n",
    "from myst_nb import glue\n",
    "\n",
    "logsdir = pathlib.Path('../data')\n",
    "\n",
    "def read_dhus_logs_details(file):\n",
    "    data = pd.read_csv(file, header=None, names=['day', 'product_type', 'action', 'volume', 'number', 'timeliness'], \n",
    "                        parse_dates=['day'], index_col=['day'])\n",
    "    return data\n",
    "\n",
    "def read_csv(file):\n",
    "    data_tmp = pd.read_csv(file, header=None, names=['product_type', 'day', 'number', 'volume'], parse_dates=['day'], sep=';')\n",
    "    # If several sensing date exist, keep the numbers only for the most recent script run (ie highest day index)\n",
    "    data_clean = copy.deepcopy(data_tmp.drop_duplicates(subset=['product_type', 'day'], keep='last')).set_index('day').sort_index()\n",
    "    data_clean['volume'] = data_clean['volume'] / 1024. / 1024\n",
    "    return data_clean\n",
    "\n",
    "data_s1 = read_dhus_logs_details(logsdir / 'S1-backend-AOI_inputs.csv')\n",
    "data_s2l1c = read_dhus_logs_details(logsdir / 'S2L1C-backend-AOI_inputs.csv')\n",
    "data_s2l2a = read_dhus_logs_details(logsdir / 'S2L2A-backend-AOI_inputs.csv')\n",
    "data_s3 = read_dhus_logs_details(logsdir / 'S3-backend-AOI_inputs.csv')\n",
    "data_s5 = read_dhus_logs_details(logsdir / 'S5p-backend-AOI_inputs.csv')\n",
    "\n",
    "all_colhub = pd.concat([data_s1, data_s2l1c, data_s2l2a, data_s3, data_s5])\n",
    "all_colhub = all_colhub[\n",
    "    (all_colhub.index <= end_date)\n",
    "]\n",
    "\n",
    "csvfile = pathlib.Path('../data/nb_products_volume_per_sensing_date.csv')\n",
    "all_netcdf = read_csv(csvfile)\n",
    "all_netcdf = all_netcdf[\n",
    "    (all_netcdf.index <= end_date)\n",
    "]\n",
    "\n",
    "alle = pd.concat([all_colhub, all_netcdf])\n",
    "\n",
    "all_vol_be = alle['volume'].sum() / 1024  # in Tb\n",
    "\n",
    "lately = alle[\n",
    "    (alle.index >= start_date) & \n",
    "    (alle.index <= end_date)\n",
    "]\n",
    "vol_be = lately['volume'].sum() / 1024  # in Tb\n",
    "\n",
    "#nb_be = be_s1 + be_s2l1 + be_s2l2 + be_s3 + be_s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc26074",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8694.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_s1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40447.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_s2l1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40625.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_s2l2"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "36265.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_s3"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15860.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_s5"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8695.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_s1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40163.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_s2l1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40340.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_s2l2"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "35771.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_s3"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15793.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_s5"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_use"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31255"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_nb"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15.122617434599306"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "col_vol"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_use"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6544"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_nb"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.2644189939546777"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "arc_vol"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11793.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "cdse_s1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40776.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "cdse_s2l1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40750.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "cdse_s2l2"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "54947.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "cdse_s3"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "cdse_s5"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue(\"col_s1\", col_s1)\n",
    "glue(\"col_s2l1\", col_s2l1)\n",
    "glue(\"col_s2l2\", col_s2l2)\n",
    "glue(\"col_s3\", col_s3)\n",
    "glue(\"col_s5\", col_s5)\n",
    "glue(\"arc_s1\", arc_s1)\n",
    "glue(\"arc_s2l1\", arc_s2l1)\n",
    "glue(\"arc_s2l2\", arc_s2l2)\n",
    "glue(\"arc_s3\", arc_s3)\n",
    "glue(\"arc_s5\", arc_s5)\n",
    "glue(\"col_use\", col_use)\n",
    "glue(\"col_nb\", col_nb)\n",
    "glue(\"col_vol\", col_vol)\n",
    "glue(\"arc_use\", arc_use)\n",
    "glue(\"arc_nb\", arc_nb)\n",
    "glue(\"arc_vol\", arc_vol)\n",
    "glue(\"cdse_s1\", cdse_s1)\n",
    "glue(\"cdse_s2l1\", cdse_s2l1)\n",
    "glue(\"cdse_s2l2\", cdse_s2l2)\n",
    "glue(\"cdse_s3\", cdse_s3)\n",
    "glue(\"cdse_s5\", cdse_s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99769f48",
   "metadata": {},
   "source": [
    "| Portals | S1 | S2L1C | S2L2A | S3 | S5p | Nb of users | Nb of products | Volume |\n",
    "|:---------:|:---------:|:--------:|:---------:|:----------:|:----------:|:-------------:|:---------:|:----------:|\n",
    "| colhub.met.no | {glue:text}`col_s1:.0f` | {glue:text}`col_s2l1:.0f` | {glue:text}`col_s2l2:.0f` | {glue:text}`col_s3:.0f` | {glue:text}`col_s5:.0f` | {glue:text}`col_use` | {glue:text}`col_nb` | {glue:text}`col_vol:.3f` |\n",
    "| colhub-archive.met.no | {glue:text}`arc_s1:.0f` | {glue:text}`arc_s2l1:.0f` | {glue:text}`arc_s2l2:.0f` | {glue:text}`arc_s3:.0f` | {glue:text}`arc_s5:.0f` | {glue:text}`arc_use` | {glue:text}`arc_nb` | {glue:text}`arc_vol:.3f` |\n",
    "| dataspace.copernicus.eu | {glue:text}`cdse_s1:.0f` | {glue:text}`cdse_s2l1:.0f` | {glue:text}`cdse_s2l2:.0f` | {glue:text}`cdse_s3:.0f` | {glue:text}`cdse_s5:.0f` |  | | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639eaf17",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Finally, the total amount of disk space dedicated to the NBS project, including either products in SAFE and NetCDF formats, represents 5845 Tb."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"Finally, the total amount of disk space dedicated to the NBS project, including either products in SAFE and NetCDF formats, represents {} Tb.\".format(int(all_vol_be)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c1d3ee",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Monthly volumes (in Tb)\n",
    "lately = alle[\n",
    "    (alle.index >= last_year) & \n",
    "    (alle.index <= end_date)\n",
    "]\n",
    "year = (lately['volume'].sum()/1024.)\n",
    "after_year = year + all_vol_be\n",
    "short = alle[\n",
    "    (alle.index >= date_123) & \n",
    "    (alle.index <= end_date)\n",
    "]\n",
    "m6 = (short['volume'].sum()/1024.)\n",
    "after_6m = m6 + all_vol_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df449b93",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Due to tracking the data ingested and produced for the NBS project in the last year it is possible to forecast the upcoming need for disk space. As long as data flows follows the same pattern than last year, in 6 months the total disk space will grow until 6206 Tb; while in 12 months it is forecast to become 6870 Tb."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"Due to tracking the data ingested and produced for the NBS project in the last year it is possible to forecast the upcoming need for disk space. As long as data flows follows the same pattern than last year, in 6 months the total disk space will grow until {} Tb; while in 12 months it is forecast to become {} Tb.\".format(int(after_6m), int(after_year)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
